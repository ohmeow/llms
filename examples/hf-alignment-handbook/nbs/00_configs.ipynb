{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import dataclasses, json\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, NewType, Optional, Tuple\n",
    "\n",
    "import transformers\n",
    "from transformers import MODEL_FOR_CAUSAL_LM_MAPPING, HfArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "\n",
    "DataClassType = NewType(\"DataClassType\", Any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.models.bart.configuration_bart.BartConfig'>, <class 'transformers.models.bert.configuration_bert.BertConfig'>, <class 'transformers.models.bert_generation.configuration_bert_generation.BertGenerationConfig'>, <class 'transformers.models.big_bird.configuration_big_bird.BigBirdConfig'>, <class 'transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig'>]\n",
      "('bart', 'bert', 'bert-generation', 'big_bird', 'bigbird_pegasus')\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_CONFIG_CLASSES[:5])\n",
    "print(MODEL_TYPES[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune.\n",
    "    \"\"\"\n",
    "\n",
    "    base_model_revision: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The base model checkpoint for weights initialization with PEFT adatpers.\")},\n",
    "    )\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\")},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    model_code_revision: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The branch of the IFT model\"},\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Trust remote code when loading a model.\"},\n",
    "    )\n",
    "    use_flash_attention_2: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to use flash attention 2. You must install this manually by running `pip install flash-attn --no-build-isolation`\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    use_peft: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": (\"Whether to use PEFT or not for training.\")},\n",
    "    )\n",
    "    lora_r: Optional[int] = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": (\"LoRA R value.\")},\n",
    "    )\n",
    "    lora_alpha: Optional[int] = field(\n",
    "        default=32,\n",
    "        metadata={\"help\": (\"LoRA alpha.\")},\n",
    "    )\n",
    "    lora_dropout: Optional[float] = field(\n",
    "        default=0.05,\n",
    "        metadata={\"help\": (\"LoRA dropout.\")},\n",
    "    )\n",
    "    lora_target_modules: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"LoRA target modules.\")},\n",
    "    )\n",
    "    lora_modules_to_save: Optional[List[str]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"Model layers to unfreeze & train\")},\n",
    "    )\n",
    "    load_in_8bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"use 8 bit precision\"},\n",
    "    )\n",
    "    load_in_4bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"use 4 bit precision\"},\n",
    "    )\n",
    "\n",
    "    bnb_4bit_quant_type: Optional[str] = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"precise the quantization type (fp4 or nf4)\"},\n",
    "    )\n",
    "    use_bnb_nested_quant: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"use nested quantization\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.load_in_8bit and self.load_in_4bit:\n",
    "            raise ValueError(\"You can't use 8 bit and 4 bit precision at the same time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model_revision': None,\n",
       " 'model_name_or_path': None,\n",
       " 'model_revision': 'main',\n",
       " 'model_code_revision': None,\n",
       " 'torch_dtype': None,\n",
       " 'trust_remote_code': False,\n",
       " 'use_flash_attention_2': False,\n",
       " 'use_peft': False,\n",
       " 'lora_r': 16,\n",
       " 'lora_alpha': 32,\n",
       " 'lora_dropout': 0.05,\n",
       " 'lora_target_modules': None,\n",
       " 'lora_modules_to_save': None,\n",
       " 'load_in_8bit': False,\n",
       " 'load_in_4bit': True,\n",
       " 'bnb_4bit_quant_type': 'nf4',\n",
       " 'use_bnb_nested_quant': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = ModelArguments(load_in_4bit=True)\n",
    "model_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_template: Optional[str] = field(default=None, metadata={\"help\": \"The chat template to use.\"})\n",
    "    dataset_mixer: Optional[Dict[str, float]] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"Datasets and their proportions to be used for training ift/rl.\")},\n",
    "    )\n",
    "    dataset_splits: Optional[List[str]] = field(\n",
    "        default_factory=lambda: [\"train\", \"test\"],\n",
    "        metadata={\"help\": (\"List of train test splits to use in the dataset\")},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\"For debugging purposes or quicker training, truncate the number of training examples to this \" \"value if set.\")\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\"For debugging purposes or quicker training, truncate the number of evaluation examples to this \" \"value if set.\")\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    truncation_side: Optional[str] = field(default=None, metadata={\"help\": \"Truncation side to use for the tokenizer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_template': 'blah',\n",
       " 'dataset_mixer': None,\n",
       " 'dataset_splits': ['train', 'test'],\n",
       " 'max_train_samples': None,\n",
       " 'max_eval_samples': None,\n",
       " 'preprocessing_num_workers': None,\n",
       " 'truncation_side': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args = DataArguments(chat_template=\"blah\")\n",
    "data_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf = transformers.TrainingArguments(\"asdf\")\n",
    "asdf.do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@dataclass\n",
    "class SFTConfig(transformers.TrainingArguments):\n",
    "    \"\"\"\n",
    "    Arguments related to the training process itself. For all parameters, see: https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    \"\"\"\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"Used by TRL for reward model training, which tries to read this parameter in init.\")},\n",
    "    )\n",
    "    logging_first_step: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": (\"Whether to log and evaluate the first global_step or not.\")},\n",
    "    )\n",
    "    optim: Optional[str] = field(default=\"adamw_torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': './blah',\n",
       " 'overwrite_output_dir': False,\n",
       " 'do_train': False,\n",
       " 'do_eval': True,\n",
       " 'do_predict': False,\n",
       " 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,\n",
       " 'prediction_loss_only': False,\n",
       " 'per_device_train_batch_size': 8,\n",
       " 'per_device_eval_batch_size': 8,\n",
       " 'per_gpu_train_batch_size': None,\n",
       " 'per_gpu_eval_batch_size': None,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'eval_accumulation_steps': None,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 5e-05,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'max_steps': -1,\n",
       " 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 0,\n",
       " 'log_level': 'passive',\n",
       " 'log_level_replica': 'warning',\n",
       " 'log_on_each_node': True,\n",
       " 'logging_dir': './blah/runs/Nov28_13-24-11_dl-rig',\n",
       " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'logging_first_step': True,\n",
       " 'logging_steps': 500,\n",
       " 'logging_nan_inf_filter': True,\n",
       " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'save_steps': 500,\n",
       " 'save_total_limit': None,\n",
       " 'save_safetensors': True,\n",
       " 'save_on_each_node': False,\n",
       " 'no_cuda': False,\n",
       " 'use_cpu': False,\n",
       " 'use_mps_device': False,\n",
       " 'seed': 42,\n",
       " 'data_seed': None,\n",
       " 'jit_mode_eval': False,\n",
       " 'use_ipex': False,\n",
       " 'bf16': False,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'half_precision_backend': 'auto',\n",
       " 'bf16_full_eval': False,\n",
       " 'fp16_full_eval': False,\n",
       " 'tf32': None,\n",
       " 'local_rank': 0,\n",
       " 'ddp_backend': None,\n",
       " 'tpu_num_cores': None,\n",
       " 'tpu_metrics_debug': False,\n",
       " 'debug': [],\n",
       " 'dataloader_drop_last': False,\n",
       " 'eval_steps': None,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'past_index': -1,\n",
       " 'run_name': './blah',\n",
       " 'disable_tqdm': False,\n",
       " 'remove_unused_columns': True,\n",
       " 'label_names': None,\n",
       " 'load_best_model_at_end': False,\n",
       " 'metric_for_best_model': None,\n",
       " 'greater_is_better': None,\n",
       " 'ignore_data_skip': False,\n",
       " 'fsdp': [],\n",
       " 'fsdp_min_num_params': 0,\n",
       " 'fsdp_config': {'min_num_params': 0,\n",
       "  'xla': False,\n",
       "  'xla_fsdp_grad_ckpt': False},\n",
       " 'fsdp_transformer_layer_cls_to_wrap': None,\n",
       " 'deepspeed': None,\n",
       " 'label_smoothing_factor': 0.0,\n",
       " 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
       " 'optim_args': None,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': False,\n",
       " 'length_column_name': 'length',\n",
       " 'report_to': ['wandb'],\n",
       " 'ddp_find_unused_parameters': None,\n",
       " 'ddp_bucket_cap_mb': None,\n",
       " 'ddp_broadcast_buffers': None,\n",
       " 'dataloader_pin_memory': True,\n",
       " 'skip_memory_metrics': True,\n",
       " 'use_legacy_prediction_loop': False,\n",
       " 'push_to_hub': False,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'hub_model_id': None,\n",
       " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
       " 'hub_token': None,\n",
       " 'hub_private_repo': False,\n",
       " 'hub_always_push': False,\n",
       " 'gradient_checkpointing': False,\n",
       " 'gradient_checkpointing_kwargs': None,\n",
       " 'include_inputs_for_metrics': False,\n",
       " 'fp16_backend': 'auto',\n",
       " 'push_to_hub_model_id': None,\n",
       " 'push_to_hub_organization': None,\n",
       " 'push_to_hub_token': None,\n",
       " 'mp_parameters': '',\n",
       " 'auto_find_batch_size': False,\n",
       " 'full_determinism': False,\n",
       " 'torchdynamo': None,\n",
       " 'ray_scope': 'last',\n",
       " 'ddp_timeout': 1800,\n",
       " 'torch_compile': False,\n",
       " 'torch_compile_backend': None,\n",
       " 'torch_compile_mode': None,\n",
       " 'dispatch_batches': None,\n",
       " 'split_batches': False,\n",
       " 'include_tokens_per_second': False,\n",
       " 'neftune_noise_alpha': None,\n",
       " 'max_seq_length': None,\n",
       " 'distributed_state': Distributed environment: DistributedType.NO\n",
       " Num processes: 1\n",
       " Process index: 0\n",
       " Local process index: 0\n",
       " Device: cuda,\n",
       " '_n_gpu': 2,\n",
       " '__cached__setup_devices': device(type='cuda', index=0),\n",
       " 'deepspeed_plugin': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_args = SFTConfig(output_dir=\"./blah\")\n",
    "train_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@dataclass\n",
    "class DPOConfig(transformers.TrainingArguments):\n",
    "    \"\"\"\n",
    "    Arguments related to the DPO training process itself. For all parameters, see: https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    \"\"\"\n",
    "\n",
    "    beta: Optional[float] = field(\n",
    "        default=0.1,\n",
    "        metadata={\"help\": \"The beta factor in DPO loss. Higher beta means less divergence from the initial policy.\"},\n",
    "    )\n",
    "    hub_model_revision: Optional[str] = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": (\"The Hub model branch to push the model to.\")},\n",
    "    )\n",
    "    logging_first_step: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": (\"Whether to log and evaluate the first global_step or not.\")},\n",
    "    )\n",
    "    max_prompt_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"For DPO, the maximum length of the prompt to use for conditioning the model.\")},\n",
    "    )\n",
    "    max_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": (\"Used by TRL for reward model training, which tries to read this parameter in init.\")},\n",
    "    )\n",
    "    optim: Optional[str] = field(default=\"rmsprop\")\n",
    "    remove_unused_columns: bool = field(default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': './blah',\n",
       " 'overwrite_output_dir': False,\n",
       " 'do_train': False,\n",
       " 'do_eval': False,\n",
       " 'do_predict': False,\n",
       " 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,\n",
       " 'prediction_loss_only': False,\n",
       " 'per_device_train_batch_size': 8,\n",
       " 'per_device_eval_batch_size': 8,\n",
       " 'per_gpu_train_batch_size': None,\n",
       " 'per_gpu_eval_batch_size': None,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'eval_accumulation_steps': None,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 5e-05,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'max_steps': -1,\n",
       " 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 0,\n",
       " 'log_level': 'passive',\n",
       " 'log_level_replica': 'warning',\n",
       " 'log_on_each_node': True,\n",
       " 'logging_dir': './blah/runs/Nov28_13-24-12_dl-rig',\n",
       " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'logging_first_step': True,\n",
       " 'logging_steps': 500,\n",
       " 'logging_nan_inf_filter': True,\n",
       " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'save_steps': 500,\n",
       " 'save_total_limit': None,\n",
       " 'save_safetensors': True,\n",
       " 'save_on_each_node': False,\n",
       " 'no_cuda': False,\n",
       " 'use_cpu': False,\n",
       " 'use_mps_device': False,\n",
       " 'seed': 42,\n",
       " 'data_seed': None,\n",
       " 'jit_mode_eval': False,\n",
       " 'use_ipex': False,\n",
       " 'bf16': False,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'half_precision_backend': 'auto',\n",
       " 'bf16_full_eval': False,\n",
       " 'fp16_full_eval': False,\n",
       " 'tf32': None,\n",
       " 'local_rank': 0,\n",
       " 'ddp_backend': None,\n",
       " 'tpu_num_cores': None,\n",
       " 'tpu_metrics_debug': False,\n",
       " 'debug': [],\n",
       " 'dataloader_drop_last': False,\n",
       " 'eval_steps': None,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'past_index': -1,\n",
       " 'run_name': './blah',\n",
       " 'disable_tqdm': False,\n",
       " 'remove_unused_columns': False,\n",
       " 'label_names': None,\n",
       " 'load_best_model_at_end': False,\n",
       " 'metric_for_best_model': None,\n",
       " 'greater_is_better': None,\n",
       " 'ignore_data_skip': False,\n",
       " 'fsdp': [],\n",
       " 'fsdp_min_num_params': 0,\n",
       " 'fsdp_config': {'min_num_params': 0,\n",
       "  'xla': False,\n",
       "  'xla_fsdp_grad_ckpt': False},\n",
       " 'fsdp_transformer_layer_cls_to_wrap': None,\n",
       " 'deepspeed': None,\n",
       " 'label_smoothing_factor': 0.0,\n",
       " 'optim': <OptimizerNames.RMSPROP: 'rmsprop'>,\n",
       " 'optim_args': None,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': False,\n",
       " 'length_column_name': 'length',\n",
       " 'report_to': ['wandb'],\n",
       " 'ddp_find_unused_parameters': None,\n",
       " 'ddp_bucket_cap_mb': None,\n",
       " 'ddp_broadcast_buffers': None,\n",
       " 'dataloader_pin_memory': True,\n",
       " 'skip_memory_metrics': True,\n",
       " 'use_legacy_prediction_loop': False,\n",
       " 'push_to_hub': False,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'hub_model_id': None,\n",
       " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
       " 'hub_token': None,\n",
       " 'hub_private_repo': False,\n",
       " 'hub_always_push': False,\n",
       " 'gradient_checkpointing': False,\n",
       " 'gradient_checkpointing_kwargs': None,\n",
       " 'include_inputs_for_metrics': False,\n",
       " 'fp16_backend': 'auto',\n",
       " 'push_to_hub_model_id': None,\n",
       " 'push_to_hub_organization': None,\n",
       " 'push_to_hub_token': None,\n",
       " 'mp_parameters': '',\n",
       " 'auto_find_batch_size': False,\n",
       " 'full_determinism': False,\n",
       " 'torchdynamo': None,\n",
       " 'ray_scope': 'last',\n",
       " 'ddp_timeout': 1800,\n",
       " 'torch_compile': False,\n",
       " 'torch_compile_backend': None,\n",
       " 'torch_compile_mode': None,\n",
       " 'dispatch_batches': None,\n",
       " 'split_batches': False,\n",
       " 'include_tokens_per_second': False,\n",
       " 'neftune_noise_alpha': None,\n",
       " 'beta': 0.1,\n",
       " 'hub_model_revision': 'main',\n",
       " 'max_prompt_length': None,\n",
       " 'max_length': None,\n",
       " 'distributed_state': Distributed environment: DistributedType.NO\n",
       " Num processes: 1\n",
       " Process index: 0\n",
       " Local process index: 0\n",
       " Device: cuda,\n",
       " '_n_gpu': 2,\n",
       " '__cached__setup_devices': device(type='cuda', index=0),\n",
       " 'deepspeed_plugin': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_args = DPOConfig(output_dir=\"./blah\")\n",
    "train_args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class H4ArgumentParser(HfArgumentParser):\n",
    "    def parse_yaml_and_args(self, yaml_arg: str, other_args: list[str] = []) -> list[dataclass]:\n",
    "        \"\"\"\n",
    "        Parse a YAML file and overwrite the default/loaded values with the values provided to the command line.\n",
    "\n",
    "        Args:\n",
    "            yaml_arg (`str`):\n",
    "                The path to the config file used\n",
    "            other_args (`List[str]`, *optional`):\n",
    "                A list of strings to parse as command line arguments, e.g. ['--arg=val', '--arg2=val2'].\n",
    "\n",
    "        Returns:\n",
    "            [`List[dataclass]`]: a list of dataclasses with the values from the YAML file and the command line\n",
    "        \"\"\"\n",
    "        arg_list = self.parse_yaml_file(os.path.abspath(yaml_arg))\n",
    "\n",
    "        outputs = []\n",
    "        # strip other args list into dict of key-value pairs\n",
    "        print(other_args)\n",
    "        other_args = {arg.split(\"=\")[0].strip(\"-\"): arg.split(\"=\")[1] for arg in other_args}\n",
    "        used_args = {}\n",
    "\n",
    "        # overwrite the default/loaded value with the value provided to the command line\n",
    "        # adapted from https://github.com/huggingface/transformers/blob/d0b5002378daabf62769159add3e7d66d3f83c3b/src/transformers/hf_argparser.py#L327\n",
    "        for data_yaml, data_class in zip(arg_list, self.dataclass_types):\n",
    "            keys = {f.name for f in dataclasses.fields(data_yaml) if f.init}\n",
    "            inputs = {k: v for k, v in vars(data_yaml).items() if k in keys}\n",
    "            for arg, val in other_args.items():\n",
    "                # add only if in keys\n",
    "                if arg in keys:\n",
    "                    base_type = data_yaml.__dataclass_fields__[arg].type\n",
    "                    inputs[arg] = val\n",
    "\n",
    "                    # cast type for ints, floats (default to strings)\n",
    "                    if base_type in [int, float]:\n",
    "                        inputs[arg] = base_type(val)\n",
    "\n",
    "                    if base_type == List[str]:\n",
    "                        inputs[arg] = [str(v) for v in val.split(\",\")]\n",
    "\n",
    "                    # bool of a non-empty string is True, so we manually check for bools\n",
    "                    if base_type == bool:\n",
    "                        if val in [\"true\", \"True\"]:\n",
    "                            inputs[arg] = True\n",
    "                        else:\n",
    "                            inputs[arg] = False\n",
    "                    \n",
    "                    # if dict, then load serialized json\n",
    "                    try:\n",
    "                        inputs[arg] = json.loads(val)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # add to used-args so we can check if double add\n",
    "                    if arg not in used_args:\n",
    "                        used_args[arg] = val\n",
    "                    else:\n",
    "                        raise ValueError(f\"Duplicate argument provided: {arg}, may cause unexpected behavior\")\n",
    "\n",
    "            obj = data_class(**inputs)\n",
    "            outputs.append(obj)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def parse(self) -> DataClassType | Tuple[DataClassType]:\n",
    "        if len(sys.argv) == 2 and sys.argv[1].endswith(\".yaml\"):\n",
    "            # If we pass only one argument to the script and it's the path to a YAML file,\n",
    "            # let's parse it to get our arguments.\n",
    "            output = self.parse_yaml_file(os.path.abspath(sys.argv[1]))\n",
    "        # parse command line args and yaml file\n",
    "        elif len(sys.argv) > 2 and sys.argv[1].endswith(\".yaml\"):\n",
    "            output = self.parse_yaml_and_args(os.path.abspath(sys.argv[1]), sys.argv[2:])\n",
    "        # parse command line args only\n",
    "        else:\n",
    "            output = self.parse_args_into_dataclasses()\n",
    "\n",
    "        if len(output) == 1:\n",
    "            output = output[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "ModelArguments(base_model_revision=None, model_name_or_path='mistralai/Mistral-7B-v0.1', model_revision='main', model_code_revision=None, torch_dtype='bfloat16', trust_remote_code=False, use_flash_attention_2=True, use_peft=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_target_modules=None, lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=False, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False)\n",
      "================================================================================\n",
      "DataArguments(chat_template=None, dataset_mixer={'HuggingFaceH4/ultrachat_200k': 1.0}, dataset_splits=['train_sft', 'test_sft'], max_train_samples=None, max_eval_samples=None, preprocessing_num_workers=12, truncation_side=None)\n",
      "================================================================================\n",
      "SFTConfig(\n",
      "_n_gpu=2,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=zephyr-7b-sft-full,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=data/zephyr-7b-sft-full/runs/Nov28_13-36-37_dl-rig,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.COSINE,\n",
      "max_grad_norm=1.0,\n",
      "max_seq_length=2048,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=data/zephyr-7b-sft-full,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=data/zephyr-7b-sft-full,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = H4ArgumentParser((ModelArguments, DataArguments, SFTConfig))\n",
    "model_args, data_args, training_args = parser.parse_yaml_and_args(yaml_arg=\"../configs/training_configs/zephyr-7b-beta/config_full_sft.yaml\")\n",
    "\n",
    "print(model_args)\n",
    "print(\"=\" * 80)\n",
    "print(data_args)\n",
    "print(\"=\" * 80)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--load_in_4bit=true', '--dataset_mixer={\"HuggingFaceH4/ultrachat_200k\": 0.25}', '--do_eval=false', '--evaluation_strategy=no']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_dir': './models/zephyr-7b-sft-lora',\n",
       " 'overwrite_output_dir': True,\n",
       " 'do_train': False,\n",
       " 'do_eval': False,\n",
       " 'do_predict': False,\n",
       " 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,\n",
       " 'prediction_loss_only': False,\n",
       " 'per_device_train_batch_size': 2,\n",
       " 'per_device_eval_batch_size': 4,\n",
       " 'per_gpu_train_batch_size': None,\n",
       " 'per_gpu_eval_batch_size': None,\n",
       " 'gradient_accumulation_steps': 128,\n",
       " 'eval_accumulation_steps': None,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 2e-05,\n",
       " 'weight_decay': 0.0,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'num_train_epochs': 1,\n",
       " 'max_steps': -1,\n",
       " 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>,\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 0,\n",
       " 'log_level': 'info',\n",
       " 'log_level_replica': 'warning',\n",
       " 'log_on_each_node': True,\n",
       " 'logging_dir': './models/zephyr-7b-sft-lora/runs/Nov28_13-53-48_dl-rig',\n",
       " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
       " 'logging_first_step': True,\n",
       " 'logging_steps': 5,\n",
       " 'logging_nan_inf_filter': True,\n",
       " 'save_strategy': <IntervalStrategy.NO: 'no'>,\n",
       " 'save_steps': 500,\n",
       " 'save_total_limit': None,\n",
       " 'save_safetensors': True,\n",
       " 'save_on_each_node': False,\n",
       " 'no_cuda': False,\n",
       " 'use_cpu': False,\n",
       " 'use_mps_device': False,\n",
       " 'seed': 42,\n",
       " 'data_seed': None,\n",
       " 'jit_mode_eval': False,\n",
       " 'use_ipex': False,\n",
       " 'bf16': True,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'half_precision_backend': 'auto',\n",
       " 'bf16_full_eval': False,\n",
       " 'fp16_full_eval': False,\n",
       " 'tf32': None,\n",
       " 'local_rank': 0,\n",
       " 'ddp_backend': None,\n",
       " 'tpu_num_cores': None,\n",
       " 'tpu_metrics_debug': False,\n",
       " 'debug': [],\n",
       " 'dataloader_drop_last': False,\n",
       " 'eval_steps': None,\n",
       " 'dataloader_num_workers': 0,\n",
       " 'past_index': -1,\n",
       " 'run_name': './models/zephyr-7b-sft-lora',\n",
       " 'disable_tqdm': False,\n",
       " 'remove_unused_columns': True,\n",
       " 'label_names': None,\n",
       " 'load_best_model_at_end': False,\n",
       " 'metric_for_best_model': None,\n",
       " 'greater_is_better': None,\n",
       " 'ignore_data_skip': False,\n",
       " 'fsdp': [],\n",
       " 'fsdp_min_num_params': 0,\n",
       " 'fsdp_config': {'min_num_params': 0,\n",
       "  'xla': False,\n",
       "  'xla_fsdp_grad_ckpt': False},\n",
       " 'fsdp_transformer_layer_cls_to_wrap': None,\n",
       " 'deepspeed': None,\n",
       " 'label_smoothing_factor': 0.0,\n",
       " 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
       " 'optim_args': None,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': False,\n",
       " 'length_column_name': 'length',\n",
       " 'report_to': [],\n",
       " 'ddp_find_unused_parameters': None,\n",
       " 'ddp_bucket_cap_mb': None,\n",
       " 'ddp_broadcast_buffers': None,\n",
       " 'dataloader_pin_memory': True,\n",
       " 'skip_memory_metrics': True,\n",
       " 'use_legacy_prediction_loop': False,\n",
       " 'push_to_hub': True,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'hub_model_id': 'zephyr-7b-sft-lora',\n",
       " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
       " 'hub_token': None,\n",
       " 'hub_private_repo': False,\n",
       " 'hub_always_push': False,\n",
       " 'gradient_checkpointing': True,\n",
       " 'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
       " 'include_inputs_for_metrics': False,\n",
       " 'fp16_backend': 'auto',\n",
       " 'push_to_hub_model_id': None,\n",
       " 'push_to_hub_organization': None,\n",
       " 'push_to_hub_token': None,\n",
       " 'mp_parameters': '',\n",
       " 'auto_find_batch_size': False,\n",
       " 'full_determinism': False,\n",
       " 'torchdynamo': None,\n",
       " 'ray_scope': 'last',\n",
       " 'ddp_timeout': 1800,\n",
       " 'torch_compile': False,\n",
       " 'torch_compile_backend': None,\n",
       " 'torch_compile_mode': None,\n",
       " 'dispatch_batches': None,\n",
       " 'split_batches': False,\n",
       " 'include_tokens_per_second': False,\n",
       " 'neftune_noise_alpha': None,\n",
       " 'max_seq_length': 2048,\n",
       " 'distributed_state': Distributed environment: DistributedType.NO\n",
       " Num processes: 1\n",
       " Process index: 0\n",
       " Local process index: 0\n",
       " Device: cuda,\n",
       " '_n_gpu': 2,\n",
       " '__cached__setup_devices': device(type='cuda', index=0),\n",
       " 'deepspeed_plugin': None}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = H4ArgumentParser((ModelArguments, DataArguments, SFTConfig))\n",
    "model_args, data_args, training_args = parser.parse_yaml_and_args(yaml_arg=\"../configs/training_configs/zephyr-7b-beta/config_lora_sft.yaml\", other_args=['--load_in_4bit=true', '--dataset_mixer={\"HuggingFaceH4/ultrachat_200k\": 0.25}', '--do_eval=false', '--evaluation_strategy=no'])\n",
    "\n",
    "model_args.__dict__\n",
    "data_args.__dict__\n",
    "training_args.__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "\n",
    "nb_export(\"00_configs.ipynb\", lib_path=\"../training_lib/\", name=\"configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
