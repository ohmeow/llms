{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin, notebook_launcher\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, PreTrainedTokenizer, set_seed\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "from configs import DataArguments, ModelArguments, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args, model_args, training_args = (\n",
    "    DataArguments(),\n",
    "    ModelArguments(),\n",
    "    SFTConfig(output_dir=\"../../models/alignment-handbook/zephyr-7b-sft-lora\"),\n",
    ")\n",
    "\n",
    "# data args\n",
    "data_args.dataset_splits = [\"train_sft\", \"test_sft\"]\n",
    "data_args.dataset_mixer = {\"HuggingFaceH4/ultrachat_200k\": 1.0}\n",
    "data_args.preprocessing_num_workers = 12\n",
    "\n",
    "# model args\n",
    "model_args.model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_args.torch_dtype = \"auto\"\n",
    "model_args.use_flash_attention_2 = True  # default = False\n",
    "# quantization\n",
    "model_args.load_in_4bit = True  # default = False\n",
    "model_args.load_in_8bit = False\n",
    "model_args.bnb_4bit_quant_type = \"nf4\"\n",
    "model_args.use_bnb_nested_quant = False\n",
    "# LoRA\n",
    "model_args.use_peft = True  # default = False\n",
    "model_args.lora_r = 64  # default = 16\n",
    "model_args.lora_alpha = 16  # default = 32\n",
    "model_args.lora_dropout = 0.1  # default = 0.05\n",
    "model_args.lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "]  # default = None\n",
    "model_args.lora_modules_to_save = None\n",
    "\n",
    "# trainer config (SFT)\n",
    "training_args.bf16 = True\n",
    "training_args.do_eval = True\n",
    "training_args.evaluation_strategy = \"epoch\"\n",
    "training_args.gradient_accumulation_steps = 128\n",
    "training_args.gradient_checkpointing = True\n",
    "training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}\n",
    "training_argshub_model_id = \"zephyr-7b-sft-lora\"\n",
    "training_argshub_strategy = \"every_save\"\n",
    "training_argslearning_rate = 2.0e-05\n",
    "training_argslog_level = \"info\"\n",
    "training_argslogging_steps = 5\n",
    "training_argslogging_strategy = \"steps\"\n",
    "training_args.lr_scheduler_type = \"cosine\"\n",
    "training_args.max_seq_length = 2048\n",
    "training_args.max_steps = -1\n",
    "training_args.num_train_epochs = 1\n",
    "training_args.output_dir = \"../../models/alignment-handbook/zephyr-7b-sft-lora\"\n",
    "training_args.overwrite_output_dir = True\n",
    "training_args.per_device_eval_batch_size = 4 #8\n",
    "training_args.per_device_train_batch_size = 2 #4\n",
    "training_args.push_to_hub = True\n",
    "training_args.report_to = \"none\" # default = [\"tensorboard\"]\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.save_total_limit = None\n",
    "training_args.seed = 42\n",
    "\n",
    "\n",
    "shuffle_datasets = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_args)\n",
    "print(\"=\"*80)\n",
    "print(model_args)\n",
    "print(\"=\"*80)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "raw_train_datasets = []\n",
    "raw_val_datasets = []\n",
    "fracs = []\n",
    "\n",
    "for ds, frac in data_args.dataset_mixer.items():\n",
    "    fracs.append(frac)\n",
    "    for split in data_args.dataset_splits:\n",
    "        if \"train\" in split:\n",
    "            raw_train_datasets.append(\n",
    "                load_dataset(\n",
    "                    ds,\n",
    "                    split=split,\n",
    "                )\n",
    "            )\n",
    "        elif \"test\" in split:\n",
    "            raw_val_datasets.append(\n",
    "                load_dataset(\n",
    "                    ds,\n",
    "                    split=split,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Split type {split} not recognized as one of test or train.\")\n",
    "\n",
    "if any(frac < 0 for frac in fracs):\n",
    "    raise ValueError(\"Dataset fractions cannot be negative.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(raw_train_datasets) > 0:\n",
    "    train_subsets = []\n",
    "    for dataset, frac in zip(raw_train_datasets, fracs):\n",
    "        train_subset = dataset.select(range(int(frac * len(dataset))))\n",
    "        train_subsets.append(train_subset)\n",
    "    if shuffle_datasets:\n",
    "        raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n",
    "    else:\n",
    "        raw_datasets[\"train\"] = concatenate_datasets(train_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No subsampling for test datasets to enable fair comparison across models\n",
    "if len(raw_val_datasets) > 0:\n",
    "    if shuffle_datasets:\n",
    "        raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)\n",
    "    else:\n",
    "        raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(raw_datasets) == 0:\n",
    "    raise ValueError(\n",
    "        f\"Dataset {data_args.dataset_mixer} not recognized with split {split}. Check the dataset has been correctly formatted.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on the following datasets and their proportions: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_TEMPLATE = \"\"\"\\\n",
    "{% for message in messages %}\n",
    "{% if message['role'] == 'user' %}\n",
    "{{ '<|user|>\\n' + message['content'] + eos_token }}\n",
    "{% elif message['role'] == 'system' %}\n",
    "{{ '<|system|>\\n' + message['content'] + eos_token }}\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\n",
    "{% endif %}\n",
    "{% if loop.last and add_generation_prompt %}\n",
    "{{ '<|assistant|>' }}\n",
    "{% endif %}\n",
    "{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_args.model_name_or_path)\n",
    "print(model_args.model_revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, revision=model_args.model_revision)\n",
    "\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id, tokenizer.truncation_side, tokenizer.model_max_length, tokenizer.chat_template, )\n",
    "print(tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "if data_args.truncation_side is not None:\n",
    "    tokenizer.truncation_side = data_args.truncation_side\n",
    "\n",
    "# Set reasonable default for models without max length\n",
    "if tokenizer.model_max_length > 100_000:\n",
    "    tokenizer.model_max_length = 2048\n",
    "\n",
    "if data_args.chat_template is not None:\n",
    "    tokenizer.chat_template = data_args.chat_template\n",
    "elif tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.eos_token, tokenizer.eos_token_id, tokenizer.truncation_side, tokenizer.model_max_length, tokenizer.chat_template, )\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template_for_sft(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    \n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "        \n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.map(apply_chat_template_for_sft, fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_datasets[\"train\"][500][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with training_args.main_process_first(desc=\"Log a few random samples from the processed training set\"):\n",
    "#     for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
    "#         print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = (\n",
    "    model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_device() -> int:\n",
    "    \"\"\"Get the current device. For GPU we return the local process index to enable multiple GPU training.\"\"\"\n",
    "    # return Accelerator().local_process_index if torch.cuda.is_available() else \"cpu\"\n",
    "    return 0\n",
    "get_current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kbit_device_map() -> dict[str, int] | None:\n",
    "    \"\"\"Useful for running inference with quantized models by setting `device_map=get_peft_device_map()`\"\"\"\n",
    "    return {\"\": get_current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "get_kbit_device_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.load_in_4bit, model_args.load_in_8bit, model_args.bnb_4bit_quant_type, model_args.use_bnb_nested_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_args.load_in_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # For consistency with model weights, we use the same value as `torch_dtype` which is float16 for PEFT models\n",
    "        bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n",
    "        bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n",
    "    )\n",
    "elif model_args.load_in_8bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "else:\n",
    "    quantization_config = None\n",
    "    \n",
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_args.model_revision, model_args.trust_remote_code, model_args.use_flash_attention_2, torch_dtype, training_args.gradient_checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    revision=model_args.model_revision,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    "    use_flash_attention_2=model_args.use_flash_attention_2,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    device_map=get_kbit_device_map(),\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_args.use_peft, model_args.lora_r, model_args.lora_alpha, model_args.lora_dropout, model_args.lora_target_modules, model_args.lora_modules_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_args.use_peft is False:\n",
    "    peft_config = None\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=model_args.lora_r,\n",
    "    lora_alpha=model_args.lora_alpha,\n",
    "    lora_dropout=model_args.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=model_args.lora_target_modules,\n",
    "    modules_to_save=model_args.lora_modules_to_save,\n",
    ")\n",
    "\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model_args.model_name_or_path,\n",
    "    model_init_kwargs=model_kwargs,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=training_args.max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    set_seed(training_args.seed)\n",
    "    \n",
    "    deepspeed_plugin = DeepSpeedPlugin(\n",
    "        offload_optimizer_device=None,\n",
    "        offload_param_device=None,\n",
    "        zero3_init_flag=True,\n",
    "        zero3_save_16bit_model=True,\n",
    "        zero_stage=3\n",
    "    )\n",
    "    \n",
    "    accelerator = Accelerator(mixed_precision=\"bf16\", deepspeed_plugin=deepspeed_plugin)\n",
    "    \n",
    "    \n",
    "    train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(train, mixed_precision=\"bf16\", num_nodes=1, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
